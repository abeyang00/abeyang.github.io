---
layout: post
title: "Learning monocular depth estimation infusing traditional stereo knowledge"
subtitle: "Learning monocular depth estimation infusing traditional stereo knowledge"
categories: data
tags: paper
comments: true
---

CVPR 2019에 accept된 "Learning monocular depth estimation infusing traditional stereo knowledge" 이라는 논문을 알아보고자 한다. 

자율자동차, 증강현실(Augmented Reality), 로보틱스같은 분야에서는 depth는 굉장히 중요한 정보다. 
LiDAR같이 3D 정보를 얻을 수 있는 센서를 사용하여 depth estimation을 진행을 하는게 인기를 받고 있지만, 카메라를 사용하여 depth estimation를 진행하는게 
점점 더 많은 각광을 받고 있다. 다른 센서들과 비교해 카메라를 사용하는게 inexpensive 할 뿐 아니라, higher resolution과 거의 어느 상황에서도 사용할 수 있다는
장점들이 있다. 

카메라를 사용해 depth estimation을 진행하는 분야에서는 stereo(2개 이상의 카메라를 사용하는)가 disparity를 추론하는데 선호된다. single camera를 사용하는 것 보다 당연하게 더 좋은 성능을 선보이지만 stereo를 사용하게 된다면 많은 pre-processing 단계가 필요하다. Stereo 장비들이 필요한것 외에도 굉장히 정확한 calibration 값들이 필요하다. 

## Monocular Residual Matching (monoResMatch)
지금부터 이 논문이 제안하는 네트워크에 대해 알아보자. MonoResMatch는 single image가 인풋으로 들어오게 되면 self-supervised manner로 정확하고 dense한 depth estimation을 하는 architecture이다. 이 네트워크는 3가지의 요소로 나눌수 있다. 
1. Multi-scale feature extractor: single raw image가 카메라로 부터 인풋으로 들어오게 되면 다양한 scale (quarter resolution부터 full resolution)의 representation을 계산한다. 
2. Initial Disparity Estimation: Multi-scale feature extractor에서 계산된 multi-scale feature maps들을 기반으로 각 scale에서 disparity map을 추출해낸다.
3. Disparity Refinement:

monoResMatch의 architecture
<img src="https://github.com/abeyang00/abeyang00.github.io/assets/img/monoResMatch_architecture.png">
이제 3가지의 요소를 좀더 자세히 알아보고자 한다.

### Multi-scale feature extractor
'Learning for disparity estimation through feature constancy'라는 논문에서 영감을 받아 single image가 인풋으로 들어오게 되면 여러개의 convolutional layers를 통해 multi-scale representations를 계산한다.
- 먼저 두개의 convolution layers를 지나게 된다. 첫번째 conv layer 는 64개의 7x7 사이즈의 filter를 stride=2로 지정하여 convolution 을 진행한다. 
  두번째 conv layer에서는 128개의 4x4 filter를 stride=2로 지정하여 convolution을 진행한다. 
- 그 다음은 앞에서 구해진 2개의 feature map을 다시 원본 사이즈로 upsample 시킨다.
- 마지막으로 upsample된 두개의 feature map을 concat 시키고 1x1 conv layer를 통해 final representation을 구하게 된다. 
결과적으로 2개의 다른 scale의 representations과 그 두개의 representations을 섞은 또 다른 representation을 ouput으로 가지게 된다.
  
### Initial Disparity Estimation
이 모듈은 앞서 'Multi-scale feature extractor' 모듈에서 구해진 multi-scale representations들을 기반으로 multi-scale ($$\frac{1}{128}$$ ~ full resolution) disparity maps들을 구하게 된다. 'A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation'에서 나온 DispNet라는 encoder-decoder를 backbone architecture로 갖는다.
- Encoder에서 각 down-sampling module은 1과 2의 stride를 갖는 2개의 conv blocks로 이루어져있다. 
  - 각 conv block에서는 3x3 사이즈의 filter를 사용하고, 사용되는 filter의 갯수를 점점 증가시킨다. (64 -> 128 -> 256 -> 512 ->1024)
  - ReLU를 non-linear activation function으로 선정하였다. 
- Decoder에서는 skip-connection을 사용해 upsampling을 진행하여 2개의 disparity maps를 output으로 갖는다.
  - 첫번째 ouput disparity map은 original input frame과 align된 disparity map
  - 두번째는 output은 'Unsupervised Monocular Depth Estimation with Left-Right Consistency'의 아이디어를 사용해 input image 의 오른쪽 가상시점 ('virtual viewpoint on its right')과 align된 disparity map
  - 각 multi-scale에 대해 위와 같이 두개의 disparity map을 추출해낸다.
이렇게 구해진 initial disparity maps들을 discontinuities와 occluded regions들에 대해 취약한 성능을 보인다. 취약한 부분을 매꾸기 위해 그 다음 step인 Disparity refinement를 진행한다.
  
### Disparity Refinement
이 모듈에서는 Initial Disparity Estimation에서 구해진 multi-scale disparity maps를 기반으로 진행된다.



