---
layout: post
title: "Learning monocular depth estimation infusing traditional stereo knowledge"
subtitle: "Learning monocular depth estimation infusing traditional stereo knowledge"
categories: data
tags: paper
comments: true
---

CVPR 2019에 accept된 "Learning monocular depth estimation infusing traditional stereo knowledge" 이라는 논문을 알아보고자 한다. 

자율자동차, 증강현실(Augmented Reality), 로보틱스같은 분야에서는 depth는 굉장히 중요한 정보다. 
LiDAR같이 3D 정보를 얻을 수 있는 센서를 사용하여 depth estimation을 진행을 하는게 인기를 받고 있지만, 카메라를 사용하여 depth estimation를 진행하는게 
점점 더 많은 각광을 받고 있다. 다른 센서들과 비교해 카메라를 사용하는게 inexpensive 할 뿐 아니라, higher resolution과 거의 어느 상황에서도 사용할 수 있다는
장점들이 있다. 

카메라를 사용해서는 Stereo

## Monocular Residual Matching (monoResMatch)
지금부터 이 논문이 제안하는 네트워크에 대해 알아보자. MonoResMatch는 single image가 인풋으로 들어오게 되면 self-supervised manner로 정확하고 dense한 depth estimation을 
하는 architecture이다. 이 네트워크는 3가지의 요소로 나눌수 있다. 
- Multi-scale feature extractor: single raw image가 카메라로 부터 인풋으로 들어오게 되면 다양한 scale (quarter resolution부터 full resolution)의 representation을
계산한다. 
- Initial Disparity Estimation: 
- Disparity Refinement:

이제 3가지의 요소를 좀더 자세히 알아보고자 한다.

### Multi-scale feature extractor
- 'Learning for disparity estimation through feature constancy'라는 논문에서 영감을 받아 single image가 인풋으로 들어오게 되면 여러개의 convolutional layer를 통해 
multi-scale representation을 계산한다.
  1. 먼저 두개의 convolution layer를 지나게 된다. 첫번째 conv layer 는 64개의 7x7 사이즈의 filter를 stride=2로 지정하여 convolution 을 진행한다. 
  두번째 conv layer에서는 128개의 4x4 filter를 stride=2로 지정하여 convolution을 진행한다. 
  2. 그 다음은 앞에서 구해진 2개의 feature map을 다시 원본 사이즈로 upsample 시킨다.
  3. 마지막으로 upsample된 두개의 feature map을 concat 시키고 1x1 conv layer를 통해 final representation을 구하게 된다.
  
### Initial Disparity Estimation



